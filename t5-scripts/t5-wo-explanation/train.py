# -*- coding: utf-8 -*-
"""t5-templatize-pytorch lightining.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZaoV6IscLxQO5GS9Pvo2_CRh02a6Cbu3
"""
import torch
import argparse
from pathlib import Path
import pytorch_lightning as pl
from pytorch_lightning.callbacks.early_stopping import EarlyStopping 
from pytorch_lightning.callbacks import ModelCheckpoint

from specific_utils import LitModel, LitOffData, TemplateHandler

# Reference: https://discuss.pytorch.org/t/weighted-cross-entropy-for-each-sample-in-batch/101358/4


# https://stackoverflow.com/questions/73314467/output-logits-from-t5-model-for-text-generation-purposes



def create_arg_parser():
    parser = argparse.ArgumentParser()
    
    parser.add_argument("--train_file", default='../../data/train.tsv', type=str,
                        help="Input file to learn from (default train.txt)")
    
    parser.add_argument("--dev_file", type=str, default='../../data/dev.tsv',
                        help="Separate dev set to read in (default dev.txt)")
        
    parser.add_argument("--learning_rate", default=1e-4, type=float,
                        help="Learning rate for the optimizer")

    parser.add_argument("--batch_size", default=32, type=int,
                        help="Batch size for training")

    parser.add_argument("--num_epochs", default=5, type=int,
                        help="Number of epochs for training")

    parser.add_argument("--max_seq_len", default=150, type=int,
                        help="Maximum length of input sequence after BPE")

    parser.add_argument("--langmodel_name", default="t5-base", type=str,
                        help="Name of the base pretrained language model")

    parser.add_argument("--ckpt_folder", default="./t5vanilla-wo-explain/", type=str,
                        help="Name of the checkpoint folder for saving the model")

    parser.add_argument("--seed", default=1234, type=int,
                        help="Seed for reproducible results")

    parser.add_argument("--device", default="gpu", type=str,
                        help="Type of device to use. gpu/cpu strict naming convention")

    args = parser.parse_args()
    return args

def main():
    args = create_arg_parser()
    print(args)

    pl.seed_everything(args.seed, workers=True)

    ckpt_folder = args.ckpt_folder
    Path(ckpt_folder).mkdir(parents=True, exist_ok=True)

    # templatehandler = TemplateHandler()
    # print("Templates used are as follows:")
    # print("="*30)
    # print(f"label mapper => {templatehandler.labelmapper}")
    # print("="*30)


    dm = LitOffData(train_file =  args.train_file,
                    dev_file =  args.dev_file,
                    batch_size = args.batch_size,
                    max_seq_len = args.max_seq_len,
                    modelname = args.langmodel_name,)


    model = LitModel(modelname = args.langmodel_name, 
                    learning_rate = args.learning_rate,
                    batch_size = args.batch_size)

    #
    early_stopping = EarlyStopping(monitor="val_loss", mode="min", patience = 3)
    checkpoint_callback = ModelCheckpoint(dirpath=ckpt_folder, monitor="val_loss",
                                        mode="min", filename="best-model")

    device_to_train = args.device if torch.cuda.is_available() else "cpu"

    trainer = pl.Trainer(deterministic=True, accelerator=device_to_train, devices=1,
                        max_epochs = args.num_epochs, fast_dev_run=False,
                        callbacks=[ early_stopping, checkpoint_callback ],
                        default_root_dir = args.ckpt_folder)

    trainer.fit(model, dm)

    print(f"Best checkpoint is {checkpoint_callback.best_model_path}")


if __name__ == '__main__':
    main()
