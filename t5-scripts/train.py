# -*- coding: utf-8 -*-
"""t5-templatize-pytorch lightining.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZaoV6IscLxQO5GS9Pvo2_CRh02a6Cbu3
"""
import json
import torch
import argparse
from pathlib import Path
import pytorch_lightning as pl
from pytorch_lightning.callbacks.early_stopping import EarlyStopping 
from pytorch_lightning.callbacks import ModelCheckpoint

from specific_utils import LitModel, LitOffData

# Reference: https://discuss.pytorch.org/t/weighted-cross-entropy-for-each-sample-in-batch/101358/4


# https://stackoverflow.com/questions/73314467/output-logits-from-t5-model-for-text-generation-purposes



def create_arg_parser():
    parser = argparse.ArgumentParser()
    
    parser.add_argument("--train_file", default='../../data/train.tsv', type=str,
                        help="Input file to learn from (default train.txt)")
    
    parser.add_argument("--dev_file", type=str, default='../../data/dev.tsv',
                        help="Separate dev set to read in (default dev.txt)")
        
    parser.add_argument("--learning_rate", default=1e-4, type=float,
                        help="Learning rate for the optimizer")

    parser.add_argument("--batch_size", default=32, type=int,
                        help="Batch size for training")

    parser.add_argument("--num_epochs", default=5, type=int,
                        help="Number of epochs for training")

    parser.add_argument("--max_seq_len", default=150, type=int,
                        help="Maximum length of input sequence after BPE")

    parser.add_argument("--max_label_len", default=30, type=int,
                        help="Maximum length of label sequence")

    parser.add_argument("--langmodel_name", default="t5-base", type=str,
                        help="Name of the base pretrained language model")

    parser.add_argument("--ckpt_folder", default="./t5model_train", type=str,
                        help="Name of the checkpoint folder for saving the model")

    parser.add_argument("--seed", default=1234, type=int,
                        help="Seed for reproducible results")

    parser.add_argument("--device", default="gpu", type=str,
                        help="Type of device to use. gpu/cpu strict naming convention")

    parser.add_argument("--dataset_name", default="svamp", type=str,
                        help="Name of the dataset to use. svamp. \n strict naming convention")

    parser.add_argument("--equation_order", default="suffix", type=str,
                        help="Order of the equation. prefix/suffix/infix. \n strict naming convention")

    # create a debug flag to run the code in debug mode which defaults to False
    parser.add_argument("--debug", default=False, action="store_true", help="Run in debug mode")

    args = parser.parse_args()
    return args

def main():
    args = create_arg_parser()
    print(args)

    pl.seed_everything(args.seed, workers=True)

    dm = LitOffData(train_file =  args.train_file,
                    dev_file =  args.dev_file,
                    batch_size = args.batch_size,
                    max_seq_len = args.max_seq_len,
                    modelname = args.langmodel_name,
                    datasetname=args.dataset_name,
                    equation_order=args.equation_order)


    model = LitModel(modelname = args.langmodel_name, 
                    learning_rate = args.learning_rate,
                    batch_size = args.batch_size)

    exp_folder =  f"{args.ckpt_folder}_{Path(args.langmodel_name).stem}_{args.dataset_name}_{args.equation_order}"

    early_stopping = EarlyStopping(monitor="val_loss", mode="min", patience = 3)
    checkpoint_callback = ModelCheckpoint(dirpath=exp_folder, monitor="val_loss",
                                        mode="min", filename="best-model")

    device_to_train = args.device if torch.cuda.is_available() else "cpu"

    if args.debug:
        debug_kwargs = dict(limit_train_batches=30, limit_val_batches=2)
    else:
        debug_kwargs = dict()

    trainer = pl.Trainer(deterministic=True, accelerator=device_to_train, devices=1,
                        max_epochs = args.num_epochs, fast_dev_run=False,
                        callbacks=[ early_stopping, checkpoint_callback ],
                        default_root_dir = exp_folder, **debug_kwargs)

    trainer.fit(model, dm)

    print(f"Best checkpoint is {checkpoint_callback.best_model_path}")
    
    model_args = vars(args)
    model_args["checkpoint_path"] = checkpoint_callback.best_model_path
    with open(f"{exp_folder}/args.json", "w", encoding="utf-8") as f:
        json.dump(model_args, f, indent=4)


if __name__ == '__main__':
    main()
